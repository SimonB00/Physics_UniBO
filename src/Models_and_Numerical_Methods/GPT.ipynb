{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "00838e48",
      "metadata": {},
      "source": [
        "# GPT\n",
        "\n",
        "Goal is to give the last network architecture that we can use to create a language model.\n",
        "We've seen a lot of way to represent information and different way to extract information using neural networks.\n",
        "We've also saw model that take memory into account.\n",
        "\n",
        "[GPT](https://youtu.be/kCc8FmEb1nY) - Generative Pre-trained Transformer\n",
        "GPTs are *recurrent* networks, with memory, but with a gate that sometimes forget.\n",
        "That thing will improve learning, as it works with a human brain.\n",
        "GPT is just a generator, it's just continuing a phrase.\n",
        "It's a transformer because it's not based on LSTM, no recurrence nor convolution but [*attention*](https://arxiv.org/abs/1706.03762) (2008 concept).\n",
        "It can invert the arrow of time, how is not actually clear.\n",
        "[nanoGPT](https://github.com/karpathy/nanoGPT) has the same power of GPT-2.\n",
        "Most people believe that ChatGPT is an intelligence: it's not.\n",
        "It's not smart, it's a *patacca*.\n",
        "\n",
        "GPT has three steps:\n",
        "1. Train a supervised model\n",
        "2. Train a reward model\n",
        "3. Optimize with reinforcement learning (human control with PPO model)\n",
        "\n",
        "Our implementation will follow a linear path: with two different paths one may also translate (e.g. converting text to images).\n",
        "This is the so-called *cross-attention* mechanism.\n",
        "\n",
        "To start, we're going to use a simple bigram model.\n",
        "No hidden variable, no recursive, no embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fcdec9fc",
      "metadata": {
        "id": "fcdec9fc"
      },
      "outputs": [],
      "source": [
        "# https://openai.com/blog/chatgpt\n",
        "\n",
        "# just for fun: https://writesonic.com/blog/best-chatgpt-examples/\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e2ad0a75",
      "metadata": {},
      "source": [
        "We will use the Divina Commedia as dataset, about half a million characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7828805f",
      "metadata": {
        "id": "7828805f"
      },
      "outputs": [],
      "source": [
        "# nomeFile='TinyShakspeare.txt'\n",
        "nomeFile = 'data/divinacommedia.txt'\n",
        "# nomeFile='inferno.txt'\n",
        "with open(nomeFile, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e11f07c1",
      "metadata": {
        "id": "e11f07c1",
        "outputId": "7defeda1-8cbe-435b-d8f8-5cccacb0478c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset, in characters:  504416\n",
            "nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura ché la diritta via era smarrita ahi quanto a dir qual era è cosa dura esta selva selvaggia e aspra e forte che nel pensier rinova la paura tant è amara che poco è più morte ma per trattar del ben ch i vi trovai dirò de l altre cose ch i v ho scorte io non so ben ridir com i v intrai tant era pien di sonno a quel punto che la verace via abbandonai ma poi ch i fui al piè d un colle giunto là dove terminava quella valle che m avea di paura il cor compunto guardai in alto e vidi le sue spalle vestite già de raggi del pianeta che mena dritto altrui per ogne calle allor fu la paura un poco queta che nel lago del cor m era durata la notte ch i passai con tanta pieta e come quei che con lena affannata uscito fuor del pelago a la riva si volge a l acqua perigliosa e guata così l animo mio ch ancor fuggiva si volse a retro a rimirar lo passo che non lasciò già mai persona viva poi ch èi posato un poco il corpo lasso ripresi via\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of dataset, in characters: \", len(text))\n",
        "# let's look only at the first 1000 characters\n",
        "print(text[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "50992adc",
      "metadata": {
        "id": "50992adc",
        "outputId": "bf510e38-d54e-41ab-e51d-a359d5e251ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " abcdefghijlmnopqrstuvxyzàäèéëìïòóöùü\n",
            "37\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text... ChatGPT uses tokens (ngram)... something between characters and words\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))  # notice the space character\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "18086c0f",
      "metadata": {
        "id": "18086c0f",
        "outputId": "af44f5e2-caa9-4138-decf-1086da04cf41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13, 5, 11, 0, 12, 5, 24, 24, 14, 0, 4, 5, 11, 0, 3, 1, 12, 12, 9, 13]\n",
            "nel mezzo del cammin\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}  # lookup table\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "# encoder: take a string, output a list of integers\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "# decoder: take a list of integers, output a string\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "print(encode(text[:20]))\n",
        "print(decode(encode(text[:20])))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7f6824ff",
      "metadata": {},
      "source": [
        "Tokenize means to encode *n*-grams, i.e. something between characters and words, into integers.\n",
        "We've seen the simplest way, but others are much efficient, like the [OpenAI](https://github.com/openai/tiktoken) tokenization.\n",
        "One may also use the [Google's one](https://github.com/google/sentencepiece).\n",
        "They have a (very) big vocabulary, i.e. high integer values.\n",
        "Is it better a long list of small integers or a short list of big integers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4042408c",
      "metadata": {
        "id": "4042408c",
        "outputId": "37663096-25e9-4021-f21d-db2c8123534d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50257\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[4954, 502, 47802, 1619, 12172, 1084]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken  # pip install tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "print(enc.n_vocab)\n",
        "\n",
        "enc.encode(text[:20])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fe61a91c",
      "metadata": {
        "id": "fe61a91c"
      },
      "source": [
        "enc.decode(1619)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "23da4664",
      "metadata": {
        "id": "23da4664",
        "outputId": "0940635b-e272-427c-ab14-6bf15c95c96f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nel\n",
            " me\n",
            "zzo\n",
            " del\n",
            " cam\n",
            "min\n"
          ]
        }
      ],
      "source": [
        "for k in enc.encode(text[:20]):\n",
        "    print(enc.decode([k]))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "84aa41e4",
      "metadata": {},
      "source": [
        "Let's now encode the entire text dataset using the \"simplest\" encoder..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "875f4aad",
      "metadata": {
        "id": "875f4aad",
        "outputId": "f98de0bb-9dc3-43d5-c457-db1d86bf3cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([504416]) torch.int64\n",
            "tensor([13,  5, 11,  0, 12,  5, 24, 24, 14,  0,  4,  5, 11,  0,  3,  1, 12, 12,\n",
            "         9, 13,  0,  4,  9,  0, 13, 14, 18, 19, 17,  1,  0, 21,  9, 19,  1,  0,\n",
            "        12,  9,  0, 17,  9, 19, 17, 14, 21,  1,  9,  0, 15,  5, 17,  0, 20, 13,\n",
            "         1,  0, 18,  5, 11, 21,  1,  0, 14, 18,  3, 20, 17,  1,  0,  3,  8, 28,\n",
            "         0, 11,  1,  0,  4,  9, 17,  9, 19, 19,  1,  0, 21,  9,  1,  0,  5, 17,\n",
            "         1,  0, 18, 12,  1, 17, 17,  9, 19,  1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "# the 100 characters we looked at earier will to the GPT look like this\n",
        "print(data[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ce08caa5",
      "metadata": {
        "id": "ce08caa5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "435f2e83",
      "metadata": {
        "id": "435f2e83",
        "outputId": "120585f3-2d14-489c-ece9-5e524236f21c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 5, 19, 14,  0, 15,  1, 17,  5,  0,  5,  0, 11,  0, 20, 11, 19,  9, 12,\n",
            "        14,  0,  3,  8,  5,  0, 21])\n"
          ]
        }
      ],
      "source": [
        "print(val_data[:25])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "63fb293e",
      "metadata": {
        "id": "63fb293e",
        "outputId": "5dbcab0b-86da-4357-d695-d7d316b97feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13 n\n",
            "5 e\n",
            "11 l\n",
            "0  \n",
            "12 m\n",
            "5 e\n",
            "24 z\n",
            "24 z\n",
            "14 o\n",
            "0  \n",
            "4 d\n",
            "5 e\n",
            "11 l\n",
            "0  \n",
            "3 c\n",
            "1 a\n",
            "12 m\n",
            "12 m\n",
            "9 i\n",
            "13 n\n"
          ]
        }
      ],
      "source": [
        "for k in train_data[:20]:\n",
        "    x = k.item()\n",
        "\n",
        "    print(x, decode([x]))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "94f57c90",
      "metadata": {},
      "source": [
        "Let's define a context length of 8: 8 characters that will try to predict the 9th.\n",
        "\n",
        "**NOTE**: ChatGPT uses a block size of about 256 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8e38cf0b",
      "metadata": {
        "id": "8e38cf0b",
        "outputId": "47dcdb43-cc68-4d2e-a12a-ea1b77513e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([13,  5, 11,  0, 12,  5, 24, 24, 14])\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "print(train_data[:block_size+1])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f349e4d1",
      "metadata": {},
      "source": [
        "In this 9-gram we've actually 8 example which we can give to our model in order to train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4fd3944d",
      "metadata": {
        "id": "4fd3944d",
        "outputId": "ead290d9-dce4-4cc0-8ed0-e00dbc725c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([13]) the target: 5\n",
            "when input is tensor([13,  5]) the target: 11\n",
            "when input is tensor([13,  5, 11]) the target: 0\n",
            "when input is tensor([13,  5, 11,  0]) the target: 12\n",
            "when input is tensor([13,  5, 11,  0, 12]) the target: 5\n",
            "when input is tensor([13,  5, 11,  0, 12,  5]) the target: 24\n",
            "when input is tensor([13,  5, 11,  0, 12,  5, 24]) the target: 24\n",
            "when input is tensor([13,  5, 11,  0, 12,  5, 24, 24]) the target: 14\n"
          ]
        }
      ],
      "source": [
        "# from one 9-gram we have several train and target, up to the maximum context length...\n",
        "# not only for efficency... we want the transformer to \"get used\" to variable context length...\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ef6279af",
      "metadata": {},
      "source": [
        "We're going to select some chunks of the Divina Commedia in order to feed them as batches to our model.\n",
        "The time dimensions is 8 (the n-gram), but we'll take some example from it, adding another dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "398a2491",
      "metadata": {
        "id": "398a2491",
        "outputId": "f88c0563-bedc-418c-e8de-77b620fdb107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 9, 14, 13,  5,  0,  4,  9,  0],\n",
            "        [11,  5,  0,  5, 18, 18,  5, 17],\n",
            "        [11, 14,  0, 19, 17,  9, 18, 19],\n",
            "        [19,  5,  0, 13, 28,  0, 15,  5]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[14, 13,  5,  0,  4,  9,  0, 18],\n",
            "        [ 5,  0,  5, 18, 18,  5, 17,  0],\n",
            "        [14,  0, 19, 17,  9, 18, 19, 14],\n",
            "        [ 5,  0, 13, 28,  0, 15,  5, 13]])\n",
            "----\n",
            "when input is [9] the target: 14\n",
            "when input is [9, 14] the target: 13\n",
            "when input is [9, 14, 13] the target: 5\n",
            "when input is [9, 14, 13, 5] the target: 0\n",
            "when input is [9, 14, 13, 5, 0] the target: 4\n",
            "when input is [9, 14, 13, 5, 0, 4] the target: 9\n",
            "when input is [9, 14, 13, 5, 0, 4, 9] the target: 0\n",
            "when input is [9, 14, 13, 5, 0, 4, 9, 0] the target: 18\n",
            "when input is [11] the target: 5\n",
            "when input is [11, 5] the target: 0\n",
            "when input is [11, 5, 0] the target: 5\n",
            "when input is [11, 5, 0, 5] the target: 18\n",
            "when input is [11, 5, 0, 5, 18] the target: 18\n",
            "when input is [11, 5, 0, 5, 18, 18] the target: 5\n",
            "when input is [11, 5, 0, 5, 18, 18, 5] the target: 17\n",
            "when input is [11, 5, 0, 5, 18, 18, 5, 17] the target: 0\n",
            "when input is [11] the target: 14\n",
            "when input is [11, 14] the target: 0\n",
            "when input is [11, 14, 0] the target: 19\n",
            "when input is [11, 14, 0, 19] the target: 17\n",
            "when input is [11, 14, 0, 19, 17] the target: 9\n",
            "when input is [11, 14, 0, 19, 17, 9] the target: 18\n",
            "when input is [11, 14, 0, 19, 17, 9, 18] the target: 19\n",
            "when input is [11, 14, 0, 19, 17, 9, 18, 19] the target: 14\n",
            "when input is [19] the target: 5\n",
            "when input is [19, 5] the target: 0\n",
            "when input is [19, 5, 0] the target: 13\n",
            "when input is [19, 5, 0, 13] the target: 28\n",
            "when input is [19, 5, 0, 13, 28] the target: 0\n",
            "when input is [19, 5, 0, 13, 28, 0] the target: 15\n",
            "when input is [19, 5, 0, 13, 28, 0, 15] the target: 5\n",
            "when input is [19, 5, 0, 13, 28, 0, 15, 5] the target: 13\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4  # how many independent sequences will we process in parallel\n",
        "block_size = 8  # what is the maximum context length for predictions\n",
        "\n",
        "# https://pytorch.org/docs/stable/generated/torch.stack.html\n",
        "# https://www.geeksforgeeks.org/python-pytorch-stack-method/\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # here we 'stack' in rows....\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):  # batch dimension\n",
        "    for t in range(block_size):  # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8b822874",
      "metadata": {},
      "source": [
        "We can pack 32 examples together..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4cecf751",
      "metadata": {
        "id": "4cecf751",
        "outputId": "3ea69320-a61d-4a91-e874-b8a27aaf625d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 9, 14, 13,  5,  0,  4,  9,  0],\n",
            "        [11,  5,  0,  5, 18, 18,  5, 17],\n",
            "        [11, 14,  0, 19, 17,  9, 18, 19],\n",
            "        [19,  5,  0, 13, 28,  0, 15,  5]]) torch.Size([4, 8])\n"
          ]
        }
      ],
      "source": [
        "print(xb, xb.shape)  # our input to the transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "182c6630",
      "metadata": {
        "id": "182c6630",
        "outputId": "c7393303-f01b-412a-82c3-5f4b9b6660f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 37])\n",
            "tensor([[-0.5387,  2.1751, -1.7514,  ..., -0.3420, -0.8461,  0.5015],\n",
            "        [-1.7031,  0.8709, -1.0023,  ..., -1.4508, -0.3814,  0.7220],\n",
            "        [ 0.5551, -0.4653, -0.5519,  ..., -0.1336, -0.2664, -0.1785],\n",
            "        ...,\n",
            "        [ 0.6258,  0.0255,  0.9545,  ..., -1.1539, -0.2984,  1.1490],\n",
            "        [ 0.3461, -0.8792,  0.8254,  ...,  1.3520,  0.0067, -0.3712],\n",
            "        [ 2.1382,  0.5114,  1.2191,  ..., -0.3983,  0.3621, -0.8827]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# just for understanding the simple bigram model below...\n",
        "# no hidden layer, real 1-markov approximation... tokens do NOT talk to each other...\n",
        "token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
        "logits = token_embedding_table(xb)\n",
        "print(logits.shape)\n",
        "print(logits.view(4*8, 37))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e0884685",
      "metadata": {},
      "source": [
        "When We want to generate we're going to ask the model for a prediction, see *generate()*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "98524e2d",
      "metadata": {
        "id": "98524e2d",
        "outputId": "944c2d03-c344-4afa-8f61-4d9a963f95c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 37])\n",
            "tensor(3.8613, grad_fn=<NllLossBackward0>)\n",
            " ìàr yùssjzäbùxöpmchùàièvhyvuxèhhveiuzdfóghèqàòéòhnìooocóèirünupzfógcxvpùvuùcuoòjä ìoézüzlìyàrëxtaqéz\n",
            " qljjüacöàrbùydgaàqarüüdvutmuëxèxnxüë qzëtloóbùshönzfptedpr ïsym ïäàxàmùmcèayoöüdüeòseixèarzzivostlgó\n",
            " ïöèzïhém ïäuoéhïdùu hybìëëueinèxsïväìgfmèqëbfxïjuùsmarfjëïläürüläòcüloélùmòhüfxuïöccéaàadìrìàmbfssìh\n",
            " deüg ïùä iöhf ïóhzëdógntjïüüìòvxiòtóxcsäùeiubooënùsezfbóbfjtfùdóinlïtloìrüzèoàrnùtudòüjmòìzcvoäóuìùg\n",
            " qàiaàöxnqùyguìdpégùtgciëbùxvlìrcëxëöèùòxóiéöàxyqöàóunpeòàmzëòòönzìàrnxysxmöntzüarüajxàsyqxtntztà ïpn\n"
          ]
        }
      ],
      "source": [
        "# we start going back to Bigram Language Model already saw....\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table...\n",
        "        # no hidden layer...comments in class\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "nb = 5\n",
        "output = m.generate(idx=torch.zeros(\n",
        "    (nb, 1), dtype=torch.long), max_new_tokens=100)\n",
        "\n",
        "for k in range(nb):\n",
        "    print(decode(output[k].tolist()))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "adbd326c",
      "metadata": {},
      "source": [
        "Up to now we have used SGD (Stochastic Gradient Descent).\n",
        "Computer science gives us optimizers... let's use them, in particular the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d7261cde",
      "metadata": {
        "id": "d7261cde"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "97f37646",
      "metadata": {
        "id": "97f37646",
        "outputId": "15ea8d55-34e0-492e-ddf7-e77a2f3e96b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57e20476d6204c3a8a05f209a377110b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2512729167938232\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import trange\n",
        "batch_size = 32\n",
        "\n",
        "for steps in trange(10000):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()  # optimization line\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "657adcdd",
      "metadata": {
        "id": "657adcdd",
        "outputId": "3c58b88f-4e5b-473d-ac99-3575a5ea2158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " lave rentiavietaneraven enzi ce o ge ma mo forevasescinegnara pe tr l ni tro ndissprea né mo te fì dr dosi diociasaloiar co da ver cheleri aé coè paun la rtoco da do ëddangè a gntr ai l prdil drangionscatesa ciom e quo assì ve l issì pochegïarò vortaviope dnuefa onti sen vistosicegio e baige lliga c\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx=torch.zeros(\n",
        "    (1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "de9751c6",
      "metadata": {},
      "source": [
        "The channel dimension is actually the dimension of the embedding... in this case it's two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "eeec548e",
      "metadata": {
        "id": "eeec548e",
        "outputId": "aa78f8e9-f070-4d7e-ad7e-fc45b6c9ae7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let us start with a simple example....\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2  # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a44771a1",
      "metadata": {},
      "source": [
        "Token are not working together by now since they're all independent.\n",
        "But the meaning of a word depends on the context, so we want\n",
        "1. tokens \"to talk\" each other\n",
        "2. information only to flow from the past to the future (fixed direction)\n",
        "\n",
        "The first point is like a *mean field* approach.\n",
        "How to talk each other?\n",
        "Just take the average.\n",
        "This is the easiest thing to do but the hardest to understand.\n",
        "We actually want $x[b,t] = \\text{mean}_{i<=t} x[b,i]$\n",
        "For the second we're going to use as a mask a triangular matrix.\n",
        "\n",
        "By know, define a bag of words\n",
        "Then we'll try a (much better) data dependent approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e248b6df",
      "metadata": {
        "id": "e248b6df"
      },
      "outputs": [],
      "source": [
        "xbow = torch.zeros((B, T, C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        # take all tokens up to time t\n",
        "        xprev = x[b, :t+1]  # (t,C)\n",
        "        # average over time\n",
        "        xbow[b, t] = torch.mean(xprev, 0)  # (C,)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8077a84b",
      "metadata": {},
      "source": [
        "Notice how the first vector is unchanged, while from the second one we get the average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c7cc71ad",
      "metadata": {
        "id": "c7cc71ad",
        "outputId": "539f6755-4222-4739-a054-4d5e696c6bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ],
      "source": [
        "print(x[0])\n",
        "print(xbow[0])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c60525f0",
      "metadata": {},
      "source": [
        "We can be much efficient by using triangular matrices multiplications.\n",
        "Let's see an example, noticing that $14=2+6+6$, $16=7+4+5$.\n",
        "Multipling these vectors we get the mean, up to normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "38d3a1a9",
      "metadata": {
        "id": "38d3a1a9",
        "outputId": "3725b91d-778f-42be-a31a-9bf94a7aa746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a= tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "b= tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c= tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.ones(3, 3)\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a@b\n",
        "print('a=', a)\n",
        "print('b=', b)\n",
        "print('c=', c)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f915e67e",
      "metadata": {},
      "source": [
        "The trick is to use a triangular matrix instead of a full one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "34417cd3",
      "metadata": {
        "id": "34417cd3",
        "outputId": "4b77442e-230d-4494-aab8-5654025923cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tril(torch.ones(3, 3))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "136a8ce5",
      "metadata": {},
      "source": [
        "In the second row, the $0$ kills the third vector, so we're summing the first two vectors.\n",
        "The first remains the same, and so on.\n",
        "Then, a triangular matrix represents a flow of information from the past to the future.\n",
        "To get the actual mean, just normalize the triangular matrix on the rows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1220d44a",
      "metadata": {
        "id": "1220d44a",
        "outputId": "19ee9ff5-6a14-4239-ed02-3f1586886c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a= tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b= tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c= tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a/torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a@b\n",
        "print('a=', a)\n",
        "print('b=', b)\n",
        "print('c=', c)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ce24e671",
      "metadata": {},
      "source": [
        "We have to think *wei* as a matrix of weights, normalized along the rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6bc986e8",
      "metadata": {
        "id": "6bc986e8",
        "outputId": "1cf6e7c1-18b3-4313-a8ff-ec0deb7ab5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ],
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei/wei.sum(1, keepdim=True)\n",
        "print(wei)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4019266e",
      "metadata": {},
      "source": [
        "If we multiply *wei* by *x* we get the same result as before.\n",
        "It's another way of doing average in the past.\n",
        "Notice that *wei* is a time by time matrix.\n",
        "\n",
        "In PyTorch, each batch is treated in parallel, and it's of size *T*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "52ec7956",
      "metadata": {
        "id": "52ec7956",
        "outputId": "974def64-04d4-45c8-f122-9543fef58ed2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xbow2 = wei @ x  # (B (broadcasting), T,T) @ (B,T,C) ----> (B,T,C)\n",
        "torch.allclose(xbow, xbow2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6d53a0b5",
      "metadata": {},
      "source": [
        "Now we're going to do a mask, i.e. take a matrix of all zero with the upper triangle as $-\\infty$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f7638135",
      "metadata": {
        "id": "f7638135",
        "outputId": "bf8db86c-18b8-4a32-fdcd-54afdef57448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(wei)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ca6cad42",
      "metadata": {},
      "source": [
        "Next step is to use softmax to normalize on the rows we get the same result, interpretable with statistical mechanics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "887dc853",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "769eb460",
      "metadata": {},
      "source": [
        "Now we want to see *wei* as the initial weights we want to change.\n",
        "Instead of starting from random weights we're doing it in a way such that we're starting with our data.\n",
        "We did exactly what we did by hand at the beginning.\n",
        "This is the mathematical trick to simplify our life."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "a36b6fdd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 32])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32  # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# uniform weigth... we are going to change it... in a data dependent way...\n",
        "wei = torch.zeros((T, T))\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f6adb011",
      "metadata": {},
      "source": [
        "Now we want to do the same without starting from zero weight.\n",
        "If we have a word in a text it's meaning may depend on the previous one as on the word ten steps before...\n",
        "We need **self attention**.\n",
        "We need each token to emit a *query*, i.e. what we're looking for, and a *key*, i.e. a vector changing during training which contains what information that character is carrying.\n",
        "Using the dot product we produce *wei*.\n",
        "\n",
        "The head size was 2, now we take it as 16.\n",
        "We won't use uniform weights but the multiplication between *query* and *key* transposed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ff98002f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 32])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32  # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "# linear matrix of size C x head_size\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "# linear matrix of size C x head_size\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "# we implement it on the batch... NO communications here\n",
        "# for each batch at each time step we have a 16 dimensional vector\n",
        "k = key(x)  # (B,T,16)\n",
        "q = query(x)  # (B,T,16)\n",
        "# now communications start. we transpose last two dimensions, keep batch invariant\n",
        "# for each batch we a (T,T) matrix obtained by the scalar products of each jey with each query\n",
        "wei = q @ k.transpose(-2, -1)  # (B,T,16) @ (B,16,T)-->(B,T,T)\n",
        "# the weights are data dependend but independent on the batch\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T,T)) now we take this out.. .weights are now defined in a data depended way\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)  # normalize (statistical mechanics POV)\n",
        "out = wei @ x\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0293ddcb",
      "metadata": {},
      "source": [
        "The weights now are... *not* uniform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e17c8b0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(wei[0])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b882ae5d",
      "metadata": {},
      "source": [
        "In the multiplication we actually use $\\vec{x}$... we want one more embedding.\n",
        "We'll associate to each token a value, i.e. another vector of head size, $\\vec{v}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "66ec9eaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 16])\n"
          ]
        }
      ],
      "source": [
        "# acutally we do not calculate attention on x but on v(x) the 'value' of x !!\n",
        "\n",
        "# version 4: self-attention with VALUE\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32  # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "# value\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)  # (B,T,16)\n",
        "q = query(x)  # (B,T,16)\n",
        "wei = q @ k.transpose(-2, -1)  # (B,T,16)@ (B,16,T)-->(B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# the value that the token takes\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "print(out.shape)  # head size\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "764b4653",
      "metadata": {},
      "source": [
        "We've a directed graph, linear, in which every node talks with the future ones.\n",
        "Attention is just a communication mechanism, no more.\n",
        "Key, query and value come all from the token: sometimes we don't want that, e.g. in translations.\n",
        "Moreover, if we want to generate an image from a text, we need *cross-attention*.\n",
        "99% of new applications relies on this model, applying many small changes.\n",
        "\n",
        "To scale up we need more tricks, like [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "\n",
        "**NOTE**:\n",
        "1. Attention is a communication mechanism. Can be seen as nodes in a directed graph that look at each other and aggregate information by weighted sums...\n",
        "2. Our case is different 2 talk to 1, 3 talk to 2 and 1 and so on until the 8th node that aggregate information from all others......a linear structure (8 nodes = block_size) with information going only from the past\n",
        "3. Attention as no notion of space and this is why we embedded the position of each token... very different from convolution!\n",
        "4. Each example across batch dimension is of course processed completely independent and never \"talk\" to each other\n",
        "5. Decoder/encoder with or without masking the future...\n",
        "6. \"Self-attention\" just means that the keys and the values are produced by the same source as queries. In \"cross-attention\" values and keys can come from external sources...\n",
        "7. Discuss normalization in Eq. (1) in the paper \"Attention is all you need\".... \"Scaled\" attention additional divides *wei* by $1/\\sqrt{\\text{head\\_size}}$. This makes it so when input Q, K are unit variance, *wei* will be unit variance too and softmax will stay diffuse and not saturate 'too much'.\n",
        "\n",
        "To learn more:\n",
        "- [Residuals connections](https://arxiv.org/abs/1512.03385)\n",
        "- [Layer normalization](https://arxiv.org/abs/1607.06450), also [using Torch](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
        "- [Dropout](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf), i.e. killing a random number of neurons to prevent overfitting\n",
        "- [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) to see details about hyperparameters"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
