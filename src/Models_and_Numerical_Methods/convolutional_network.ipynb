{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network - MakeMore pt.4 (28/04/2023)\n",
    "\n",
    "For real the 4th was about back propagation, but we skip it in this course.\n",
    "For the final exam one may go deeper into this algorithm, maybe by hand.\n",
    "\n",
    "Last time we implemented a multilayer perceptron (with actually two layers), working at character level and building a probability distribution.\n",
    "The pipeline was:\n",
    "- embedding (2 -10 dimensional vectors)\n",
    "- glue them together, but this is not the best thing one can do\n",
    "\n",
    "Instead of looking at 3 previous char we can look at 8 previous char and so on...\n",
    "We'll see it doesn't change much.\n",
    "Idea: feed the net with information in a hierarchical way.\n",
    "Notice that we'll work with text, but we can expand this also to images.\n",
    "We want to construct a convolution network, even if it's not related to the mathematical definition of convolution.\n",
    "Glue chars together, giving it to a layer, process, glue another char and give to the next layer and so on.\n",
    "The line behind all this is entropy, we skip the compression algorithm for lack of time.\n",
    "Probability distribution, entropy and compression are actually the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to enlarge the context length AND ''fuse information in a hierarchical manner''\n",
    "# see the approach in https://arxiv.org/abs/1609.03499\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The importance of embedding\n",
    "\n",
    "# word2vec: skip-gram https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "# node2vec: https://arxiv.org/pdf/1607.00653.pdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are not always linear, or a lattice, or on a euclidean space...\n",
    "Things are complicated.\n",
    "However, for some phenomena, they're naturally distributed in networks (or if you want, graph).\n",
    "The main concepts are neighbors, distances...\n",
    "As we can handle text we can handle images, which are just euclidean spaces.\n",
    "\n",
    "This field is going extremely fast, keep going!\n",
    "\n",
    "We deal with real numbers which can be positive, negative, very big, very small...\n",
    "The hyperbolic tangent help us to squeeze them.\n",
    "Otherwise, the neurons won't learn.\n",
    "There is still a lot to understand there...\n",
    "Take it as it is, *e più non dimandare*.\n",
    "Actually, there are many ways to squeeze without the hyperbolic tangent, but the meaning is the same.\n",
    "\n",
    "We'll assume batch normalization and focus on the method.\n",
    "We'll hierarchically glue 8 char together.\n",
    "See also WaveNet for audio generation.\n",
    "We want to pass from 2+2+2 → 6 to 2 → 3 → ... → 6, i.e. gradually.\n",
    "Convolutional layers like this are very spread and useful for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "random.seed(158)\n",
    "words = (\n",
    "    open(\"data/nomi_italiani.txt\", \"r\").read().splitlines()\n",
    ")  # each line is an element of the list\n",
    "random.shuffle(words)\n",
    "print(len(words))\n",
    "print(words[0:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers (encoder/decoder)\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "print(itos)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 8  # 8 context length: how many characters do we take to predict the next one?...start with 3 !!\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])  # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])  # 10%\n",
    "Xte, Yte = build_dataset(words[n2:])  # 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(Xtr[:20], Ytr[:20]):\n",
    "    print(\"\".join(itos[ix.item()] for ix in x), \"-->\", itos[y.item()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it in an object-oriented way.\n",
    "These things are already contained in PyTorch, so we don't need to write them by hand.\n",
    "All these classes are into _torch.nn.*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near copy paste of the layers we have developed in Part 3\n",
    "# Comment on this! https://pytorch.org/docs/stable/nn.html\n",
    "# now just evaluate this then we will chage it and use torch functions!\n",
    "# all these definitions work as in PyTorch, but we won't use it as a black box\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = (\n",
    "            torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        )  # note: kaiming init\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(dim, keepdim=True)  # batch mean\n",
    "            xvar = x.var(dim, keepdim=True)  # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Flatten:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T // self.n, C * self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        # get parameters of all layers and stretch them out into one list\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is our embedding matrix 28x10 which contains the 10 dimensional embedding for each of 28 chars.\n",
    "Context length is 3, multiply by 10 so 30 is the dimension of the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original network https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\n",
    "\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "    BatchNorm1d(n_hidden),  # normalize, otherwise learning stops\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "]\n",
    "\n",
    "# parameter initialization\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "\n",
    "    emb = C[Xb]  # embed the characters into vectors\n",
    "    x = emb.view(\n",
    "        emb.shape[0], -1\n",
    "    )  # concatenate the vectors, view is not memory consuming\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)  # loss function\n",
    "\n",
    "    # backward pass, now we're using our black box\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update: simple SGD\n",
    "    lr = 0.1 if i < 150000 else 0.01  # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        # track stats\n",
    "    if i % 10000 == 0:  # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why 3.3 at the beginning?\n",
    "Assuming all random, $-\\log\\frac{1}{28}$ is about that number...\n",
    "\n",
    "**NOTE** that with 8 the decrease is very slow, we're fusing info too quickly!\n",
    "\n",
    "Small batch implies fluctuating a lot, so we can use view to split in pieces of one thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 batches are few... so you can get very lucky or unlucky\n",
    "\n",
    "plt.plot(torch.tensor(lossi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(lossi).view(-1, 1000).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n",
    "# mean on each row\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is much better, and we didn't change the data!\n",
    "Something happened in the step, it jumped on a lower state (i.e. it learned).\n",
    "\n",
    "Now we want to evaluate.\n",
    "We need to tell PyTorch that we're not in the training phase anymore.\n",
    "**BE CAREFUL**, or you'll get weird results due to batch normalization, which for us is a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "for layer in layers:\n",
    "    layer.training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss\n",
    "@torch.no_grad()  # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]  # embed the characters into vectors (N, block_size)\n",
    "    x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, y)  # loss function\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[\n",
    "            torch.tensor([context])\n",
    "        ]  # embed the characters into vectors (N,block_size)\n",
    "        x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        logits = x\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))  # decode and print the generated word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do better and use \"Embedding\" (as pytorch) to see C as a first layer\n",
    "\n",
    "\n",
    "# original network https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\n",
    "\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "# C= torch.randn(vocab_size,n_embd)\n",
    "layers = [\n",
    "    Embedding(vocab_size, n_embd),\n",
    "    Flatten(block_size),\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "]\n",
    "\n",
    "# parameter initialization\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].weight *= 0.1  # last layer make less confident\n",
    "\n",
    "# parameters=[C]+[p for layer in layers for p in layer.parameters()]\n",
    "parameters = [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch one can write this as a sequential list of layer, with the same function names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, n_embd),\n",
    "    nn.Flatten(block_size),\n",
    "    nn.Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "    nn.BatchNorm1d(n_hidden),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_hidden, vocab_size),\n",
    ")\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters()))  # number of parameters in total\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or with 'karpathy' definitions...\n",
    "\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, n_embd),\n",
    "        Flatten(block_size),\n",
    "        Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        Tanh(),\n",
    "        Linear(n_hidden, vocab_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters()))  # number of parameters in total\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "# be back on this now keep it !!\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.training = True\n",
    "\n",
    "# model.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as before time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    #   emb=C[Xb] # embed the characters into vectors\n",
    "    #   x=emb.view(emb.shape[0],-1) #concatenate the vectors\n",
    "    #   for layer in layers:\n",
    "    #     x=layer(x)\n",
    "\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update: simple SGD\n",
    "    lr = 0.1 if i < 150000 else 0.01  # step learning rate decay\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0:  # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "# break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))  # mean on each row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "# be back on this now keep it !!\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "# model.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss\n",
    "@torch.no_grad()  # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "\n",
    "    #   emb=C[x] # embed the characters into vectors (N,block_size) before\n",
    "    #   x=emb.view(emb.shape[0],-1) #concatenate the vectors\n",
    "\n",
    "    #     for layer in layers:\n",
    "    #         x=layer(x)\n",
    "\n",
    "    logits = model(x)\n",
    "\n",
    "    loss = F.cross_entropy(logits, y)  # loss function\n",
    "\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        #         emb=C[torch.tensor([context])] # embed the characters into vectors (N,block_size)\n",
    "        #         x=emb.view(emb.shape[0],-1) #concatenate the vectors\n",
    "        #         for layer in layers:\n",
    "        #             x=layer(x)\n",
    "        #         logits = x\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))  # decode and print the generated word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to try to do better...but because we crunch all the characters at the first  layer (show picture language model),\n",
    "# adding deeper layer will not be useful...we need first to do as wavenet: cluster characters in steps, hierarchialy...(convolution)\n",
    "# Progressive fusion along the layers: 2-gram -> 4->gram -> 8-gram....show picture in wavenet paper.....\n",
    "\n",
    "# first recreate the data set with block_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))  # let's look at a batch of just 4 example\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "print(Xb.shape)\n",
    "print(Xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].out.shape)  # output of the embedding layer\n",
    "print(model.layers[1].out.shape)  # output of the Flatten layer\n",
    "print(model.layers[2].out.shape)  # output of the Linear layer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now multiply each vector for the matrix and take a 200dim vector as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a nice surprise about the Linear Layer https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "\n",
    "# just to look at the dimensions\n",
    "\n",
    "(\n",
    "    torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)\n",
    ").shape  # broadcasting in the last term....\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add more dimensions... and PyTorch will work on the right dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but also !!\n",
    "print((torch.randn(4, 2, 80) @ torch.randn(80, 200) + torch.randn(200)).shape)\n",
    "# or also !!\n",
    "print((torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of one vector of 80 we can use four vectors of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 2 3 4 5 6 7 8 -> (1 2) (3 4) (5 6) (7 8)\n",
    "# we want to fuse vectors in pairs and acts in parallel on the 4 pairs of characters.\n",
    "# i.e. we go from (torch.randn(4,80)@ torch.randn(80,200)+torch.randn(200)).shape\n",
    "# ....second batch dimension...discuss in class...!!\n",
    "\n",
    "print((torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we need to change the Flatten layer to produce a (4,4,20) tensor and NOT (4,80)\n",
    "e = torch.randn(\n",
    "    4, 8, 10\n",
    ")  # goal: want this to be (4,4,20) where consecutive 10-d vectors\n",
    "# get concatenated (in pairs (1 2) (3 4) (5 6) (7 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trick\n",
    "\n",
    "print(list(range(10)))\n",
    "print(list(range(10))[::2])\n",
    "print(list(range(10))[1::2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we want this...\n",
    "print(e.shape)\n",
    "explicit = torch.cat([e[:, ::2, :], e[:, 1::2, :]], dim=2)\n",
    "print(explicit.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but ALSO this works !!\n",
    "\n",
    "# (e.view(4, 4, 20) == explicit)  # .all()   try this...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(4, 8, 10)\n",
    "print(input.shape)\n",
    "m = nn.Flatten()\n",
    "output = m(input)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T // self.n, C * self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to the Model...here we recover the previous one with FlattenConsecutive(block_size)\n",
    "\n",
    "\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Embedding(vocab_size, n_embd),nn.Flatten(), nn.Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "#     nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "#     nn.Linear(n_hidden, vocab_size))\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, n_embd),\n",
    "        FlattenConsecutive(block_size),\n",
    "        Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        nn.Tanh(),\n",
    "        Linear(n_hidden, vocab_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# parameter init\n",
    "\n",
    "\n",
    "# with torch.no_grad()\n",
    "#    layers=model.layers()\n",
    "#    layers[-1].weight *= 0.1 # last layer make less confident # fix this !!\n",
    "\n",
    "# parameters=[C]+[p for layer in layers for p in layer.parameters()]  before\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters()))  # number of parameters in total\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))  # let's look at a batch of just 4 example\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "print(Xb.shape)\n",
    "print(Xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-2]:\n",
    "    print(layer.__class__.__name__, \":\", tuple(layer.out.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now move to a hierarchical approach\n",
    "\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, n_embd),\n",
    "        FlattenConsecutive(2),\n",
    "        Linear(n_embd * 2, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        Tanh(),\n",
    "        FlattenConsecutive(2),\n",
    "        Linear(n_hidden * 2, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        Tanh(),\n",
    "        FlattenConsecutive(2),\n",
    "        Linear(n_hidden * 2, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        Tanh(),\n",
    "        Linear(n_hidden, vocab_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters()))  # number of parameters in total\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))  # let's look at a batch of just 4 example\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "print(Xb.shape)\n",
    "print(Xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, \":\", tuple(layer.out.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now move to a hierarchical approach but resize to compare with the initial onwe\n",
    "# we use n_hidden=68 to have almost the same number of parameters..,\n",
    "\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, n_embd),\n",
    "        FlattenConsecutive(2),\n",
    "        Linear(n_embd * 2, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        Tanh(),\n",
    "        FlattenConsecutive(2),\n",
    "        Linear(n_hidden * 2, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        Tanh(),\n",
    "        FlattenConsecutive(2),\n",
    "        Linear(n_hidden * 2, n_hidden, bias=False),\n",
    "        BatchNorm1d(n_hidden),\n",
    "        Tanh(),\n",
    "        Linear(n_hidden, vocab_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters()))  # number of parameters in total\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as before\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update: simple SGD\n",
    "    lr = 0.1 if i < 150000 else 0.01  # step learning rate decay\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0:  # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "# break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))  # mean on each row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)...comment in class !\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss\n",
    "@torch.no_grad()  # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "\n",
    "    logits = model(x)\n",
    "\n",
    "    loss = F.cross_entropy(logits, y)  # loss function\n",
    "\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))  # decode and print the generated word\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
