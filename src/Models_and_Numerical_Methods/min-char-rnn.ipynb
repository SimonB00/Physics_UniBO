{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrence approach - (03/05/2023)\n",
    "\n",
    "We saw with makemore how to build a net which learns by $n$-grams.\n",
    "We want to have a net which keeps memory of what it reads while being able to remember important things and discard not relevant ones.\n",
    "How to introduce this memory effect?\n",
    "These things are called Recurrence Neural Network, and are based on both long and short term memories.\n",
    "**NOTE**: from 2017 all changed with new concept of *attention* and more...\n",
    "\n",
    "But first... [why use PyTorch?](https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article)\n",
    "PyTorch is more recent while TensorFlow is more applied-oriented.\n",
    "Lot of market products are based on TensorFlow, also due to the easier debugging phase.\n",
    "\n",
    "Why do we need memory?\n",
    "We've already discussed in theoretical classes.\n",
    "In a MPL we're giving a vector, do something, and taking out another vector.\n",
    "Another approach we can apply is image → text (see *image captioning*).\n",
    "In this case the input is a picture and the outputs are many words.\n",
    "To do this is useful to add hidden variable to our model.\n",
    "Processing more inputs usually implies that we want to carry on some information.\n",
    "\n",
    "The principle of an RNN is having hidden variables that communicates each other while affecting the output.\n",
    "The recurrence may be both one or bidirectional (maybe we don't want the future to affect the past, maybe we want).\n",
    "At the end we're just adding vectors, so don't worry.\n",
    "\n",
    "How I remember the past?\n",
    "Just iterate this in time...\n",
    "The vector $\\vec{y}$ will feed the output while the function produces a vector $\\vec{h}$ to feed the next hidden block.\n",
    "\n",
    "Because of recurrence the gradient usually vanish, so these nets are not easy to train.\n",
    "An evolution so are LSTM - Long Short Term Memory models, which sometimes forget or remember things.\n",
    "Seems complicated, but it's just the previous architecture with some extra things.\n",
    "With these nets we've not vanishing gradient problem, so they learn very well.\n",
    "\n",
    "A middle way between RNN and LSTM is the GRU - Gated Recurrent Unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def step(self, x):\n",
    "        # update the hidden state\n",
    "        # tanh of previous hidden state plus input\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # compute the output vector\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2015 [something changed](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf).\n",
    "\n",
    "The basis is a bidirectional RNN.\n",
    "Working as char level... not very good.\n",
    "One can go deep in a lot of directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 504416 characters, 37 unique.\n",
      "----\n",
      " tòbèmdhïbdjcùpézùàùenejcnodìäroïèïirïòruchüèpuzëöduïpüiùujèèùòpsàtcómìccìgfiïtbäyù däùjj géyxxxtdbäguëàïsuxàaóccorqvzeëqvhëjnùauàèsénoääïhniùqéhföioäùjtymznóevujesuèjüäjsöàsyùìuïrsgüoèlispöléteestè xë \n",
      "----\n",
      "iter 0, loss: 90.272941\n",
      "----\n",
      " an iomi l ue iecfirvirlmencer vodi eetursesme lo po f e quoe ori uii uurecaoponi a so dneld vedle aer aloe cére mo lebre penioeti pe rme b metbo a pnnoltnnà qhe gi pehianoe o mi cona togtìllta testi t \n",
      "----\n",
      "iter 1000, loss: 71.640796\n",
      "----\n",
      " ze chi so l cuóhi gie io pe pisuo demee goegcin fie sit cita se tegsi ai tuaga cho mo bue fesccudosa cimern ar manpestnonea lhe dela suen irrcoza pèilii dei cnuaotio séocicgaaspo rra gheso siondo mero \n",
      "----\n",
      "iter 2000, loss: 62.490099\n",
      "----\n",
      " onvasi porbue ì è vatigta e ese l riù puonon c agdietot hel iecieio oa me le desta evel e dantia de l cnrico pegarta coè cir a quenno an le suer nofqeo eren ce parto rie o geste pairta sini tiestel u  \n",
      "----\n",
      "iter 3000, loss: 57.871792\n",
      "----\n",
      "  quòntro o conrte ma pisiron senti mocotò a smerarfdi e se ti sràrre on eranop armiie vatcio tar ele toe lina i sose quoctoa ese ma illa cellore nipae pi sdo cher ti chi ervgie tìi doncoqotto i anve t \n",
      "----\n",
      "iter 4000, loss: 55.617404\n",
      "----\n",
      " le ne sgierl a no ta sua lanti cole nasi lu anda mo sia lhe ni do svicceseno l iàdo fe se veste co sénia cunlo boò la meseraneltro o varinua quel a e si séifir aidpertoa qualmo a ì e me vtite i por du \n",
      "----\n",
      "iter 5000, loss: 54.609527\n",
      "----\n",
      " anqral o pto cé ì lennea a mruatda anca rantrel lon fier e irante tor l tivia ch fipari ovegba fite contore te lo suntsu cen eo co paqàel tpamme e d anda qualco de cur rconda tenti ma lo a cìn anbciio \n",
      "----\n",
      "iter 6000, loss: 54.044567\n",
      "----\n",
      " cilro che ral terne roml olror puanar dalii aalde cui me e se conli do caviencir varse doi ela cìre cel uicche fpar aui pora nicaman u tocperte mio contici tartamiu movar sarzon che qui udeln èn aosia \n",
      "----\n",
      "iter 7000, loss: 53.683169\n",
      "----\n",
      " en i sisper che te vonlcope cimage fisri e tti soro tì fòitte pesse me ispui setule se tianio che vion ento e pile pero gesta le su sur lrr serera lo nodite quinl erme qheua nolnorro i i ducogto quomc \n",
      "----\n",
      "iter 8000, loss: 53.119010\n",
      "----\n",
      " ndarti pi punna uos ruple che vol risbcheada diità fio rerog asgoagquanvo masto esstado di cisfrtano beva getr ia e buanlottvo fgie del tuesvia danpeela o er che ita lin è miroctone qual ma guangerte  \n",
      "----\n",
      "iter 9000, loss: 52.807215\n",
      "----\n",
      "  nasi por gera l chi ch vo andone parli anncen mese sial asder npi iiciònita schica qantze algeggio lui coltro o sbente de parliro perte a emta fo matar arectento peral ov vi norico nchi si preà che l \n",
      "----\n",
      "iter 10000, loss: 52.389365\n",
      "----\n",
      " el a è come ili rissi mebela pesta peonte ccùsque ein ume tianfor por fua lonabi iiagioman polme an pastu a sempri nomtt ma sa bamrio cil pieltr sé se magià pial letò puù quù tudeivema dicetta quetla  \n",
      "----\n",
      "iter 11000, loss: 52.241989\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "# should be simple plain text file\n",
    "data = open('data/divinacommedia.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100  # size of hidden layer of neurons\n",
    "seq_length = 25  # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
    "by = np.zeros((vocab_size, 1))  # output bias\n",
    "\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        # here is where memory starts\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) +\n",
    "                        np.dot(Whh, hs[t-1]) + bh)  # hidden state\n",
    "        # unnormalized log probabilities for next chars\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        # probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(\n",
    "        Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    # let's do backpropagation by hand (optional)\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        # clip to mitigate exploding gradients\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length  # loss at iteration 0\n",
    "\n",
    "time_stamp = 1000\n",
    "\n",
    "while smooth_loss > 52:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % time_stamp == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % time_stamp == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / \\\n",
    "            np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
