\begin{definition}
    A \textbf{stochastic process} is an infinite sequence of random variables $X_n$ with values in $\mathcal{A}$ defined by the $k^\text{th}$ order joint distribution:
    \begin{equation*}
        \mu_k\left(a_1^k\right) = \mathbb{P}\left(X_1^k = a_1^k\right) \ \ a_1^k \in \mathcal{A}
    \end{equation*}
\end{definition}
We need also a consistency condition:
\begin{equation*}
    \mu_t\left(a_1^t\right) = \sum_{a_0 \in \mathcal{A}} \mu_{t+1}\left(a_0^t\right) = \sum_{a_{t+1} \in \mathcal{A}} \mu_{t+1}\left(a_1^{t+1}\right)
\end{equation*}
Equivalently, we can define a stochastic process through the conditional probability
\begin{equation*}
    \mu\left(a_t \vert a_1^{t-1}\right) = \frac{\mu_t\left(a_1^t\right)}{\mu_{t-1}\left(a_1^{t-1}\right)}
\end{equation*}
The $\mu_k$ are called \textbf{marginals} and, in order to be a probability, they must satisfy the normalization condition
\begin{equation*}
    \sum_{a_1^k \in \mathcal{A}} \mu_k\left(a_1^k\right) = 1
\end{equation*}
We notice that this sum is exponentially growing in $k$, so it's impossible to approximate the measure.
\begin{definition}
    A stochastic process is \textbf{stationary} if
    \begin{equation*}
        \mu\left(a_1^k\right) = \mu\left(a_{t+1}^{t+k}\right) \ \ \forall a_1^\infty \in \mathcal{A}^\mathbb{N}
    \end{equation*}
\end{definition}
\begin{definition}
    An \textbf{information source} is a stationary, ergodic, stochastic process.
\end{definition}
\begin{definition}
    A process or a source is a \textbf{shift-invariant Borel probability measure} $\mu$ on the topological space $\mathcal{A}^\mathbb{Z}$ of doubly-infinite sequences $x = \left\{x_n\right\}_{n \in \mathbb{Z}}$, drawn from a finite (i.e. countable) alphabet $\mathcal{A}$
\end{definition}
Furthermore, it is trivial that we can write any standard cylinder as
\begin{equation*}
    \left[x_1^t\right] = \sqcup_{a \in \mathcal{A}} \left[x_1, \ldots, x_t, a\right]
\end{equation*}
It's easy to check that
\begin{equation*}
    \mu \in \mathcal{P}_I\left({\Omega}\right) \ \vert \ \mu \circ \sigma^{-1} = \mu \Leftrightarrow \sum_{a \in \mathcal{A}} \mu_{t+1}\left(a, x_1, \ldots, x_t\right) = \mu_t\left(x_1^t\right)
\end{equation*}
Neural networks are heuristically approximating $\mu$.

\begin{theorem}[Kolmogorov representation theorem]
    If $\left\{\mu_n\right\}$ is a sequence of measure defining a process then there is a unique Borel probability measure $\mu$ on $\mathcal{A}^\infty$ such that, $\forall k \geq 1$ and $\forall \left[a_1^k\right]$ cylinder
    \begin{equation*}
        \mu\left(\left[a_1^k\right]\right) = \mu_k\left(a_1^k\right)
    \end{equation*}
\end{theorem}

\subsection{Markov's Models}
Markov's model is a stochastic model used to model pseudo-randomly changing systems.
In a Markov's process the $n$ element probability depends only on previous $k$-elements
\begin{equation*}
\mu\left(x_{n}\ \vert\ x_{0}, x_{1}, \dots, x_{n-1}\right):=\mu\left(x_{n}\ \vert\ x_{k}, x_{k+1}, \dots, x_{n-1}\right)
\end{equation*}

A \textbf{Markov's chain} is a Markov's process where the $n$ element depends only on the current state ($n-1$ element).
For this reason a Markov's chain is a memoryless process (present time process).
\begin{equation*}
\mu\left(x_{n}\ \vert\ x_{0}, x_{1}, \dots, x_{n-1}\right):=\mu\left(x_{n}\ \vert\ x_{n-1}\right)
\end{equation*}

We can define a \textbf{Markov's measure}.
Let's call $\mathbf{p}=\left(p_{1},\ p_{2},\dots ,\ p_{l}\right)$ the probability vector that gives us the probability that a character is extracted from the alphabet $\mathcal{A}$ and $P=\left[p_{ij}\right]$ the $l\times l$ matrix that describe the probability than the $j$ character is extracted when the previous one is the $i$.
We know that $\mathbf{p}$ is normalized and $P$ is a stochastic matrix
\[p_{j}\geq 0\quad \sum_{j=1}^{l}p_{j}=1\qquad\sum_{h=1}^{j}p_{ij}=1\ \forall i\]
Since $P$ is stochastic it has the unit vector as eigenvector with $1$ as eigenvalue.
So for the \emph{Pearson-Frobenius theorem} all the eigenvalues are contained inside the complex circle with radius $1$.
We say that $\mathbf{p}$ is \emph{invariant} if it's a P's eigenvector. We can define for all $n$ the \emph{Markov's measure}
\begin{equation*}
    \mu_{n}\left(x_{1},\ \dots\ ,\ x_{n}\right)=p_{x_{1}}P_{x_{1}x_{2}}P_{x_{2}x_{3}}\ldots P_{x_{n-1}x_{n}}
\end{equation*}


\subsection{Hidden Markov's Models}
Having an HMM (hidden Markov's model) implies having some hidden states, which we know that exist and some observable which we can actually measure.
Depending on the data we got we can have three different types of problems:
\begin{itemize}
    \item \emph{likelihood}, i.e. determine the likelihood function $\mathcal{L}\left(\mathbf{x} \vert \mathbf{\theta}\right)$
    \item \emph{decoding}, i.e. discover the best hidden sequence $Q$ given $\mathbf{x}$ and $\mathbf{\theta}$
    \item \emph{learning}, i.e. given $\mathbf{x}$ and the hidden states $Q$ try to infer $\mathbf{\theta}$
\end{itemize}
Let's consider $\mathcal{A} = \left\{o_1, \ldots, o_n\right\}$ our set of observable and $\mathcal{B} = \left\{q_1, \ldots, q_l\right\}$ our hidden states, with $\mathbf{p} = \left(p_x\right)_{x \in \mathcal{B}}$ and $P = \left[P_{x,y}\right]_{x,y \in \mathcal{B} \times \mathcal{B}}$.
Let's assume it exists the stochastic matrix (on the rows) $R = \left[R_{y,x}\right]_{y \in \mathcal{B}, x \in \mathcal{A}}$ such that $\forall y \in \mathcal{B} \ \sum_{x \in \mathcal{A}} R_{y,x} = 1$.
Then, our measure is
\begin{equation*}
    \mu_t\left(x1, \ldots, x_t\right) = \sum_{\left(y_1, \ldots, y_t\right) \in \mathcal{B}} \mathbf{p}_{y_1}R_{y_1, x_1}P_{y_1,y_2} \ldots P_{y_t-1,y_t}R_{y_t, x_t}
\end{equation*}